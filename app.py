# app.py â€” å®Œæ•´è¦†è“‹ç‰ˆï¼ˆçˆ¬èŸ²å¯« webcommentIT.commentï¼›åˆ†æè®€ webcommentIT_train.comment_train A1~A10ï¼›ç‹€æ…‹æŒä¹…åŒ–ï¼›æ’ç¨‹å€’æ•¸ APIï¼‰

import os
import io
import re
import csv
import sys
import json
from glob import glob
from threading import Thread
from typing import Optional, List, Dict, Any
from datetime import datetime, date, timedelta

# å…ˆè¼‰å…¥ .envï¼ˆä¸€å®šè¦åœ¨è®€å– os.environ å‰ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv(override=True)
except Exception:
    pass

from flask import (
    Flask, render_template, redirect, url_for, flash,
    request, jsonify, send_file, session, render_template_string
)
from flask_apscheduler import APScheduler
from pymongo import MongoClient, ASCENDING, TEXT

# =========================
# Flask
# =========================
app = Flask(__name__)
app.secret_key = os.environ.get("FLASK_SECRET_KEY", "change-me-secret")

# =========================
# MongoDBï¼ˆä¸»åº«ï¼šçˆ¬èŸ²å¯«å…¥ç”¨ï¼‰
# =========================
MONGO_URL  = os.environ.get("MONGO_URL", "mongodb://localhost:27017")
MONGO_DB   = os.environ.get("MONGO_DB", "webcommentIT")
MONGO_COLL = os.environ.get("MONGO_COLL", "comment")

mongo_client = MongoClient(MONGO_URL, tz_aware=True)
db   = mongo_client[MONGO_DB]
coll = db[MONGO_COLL]

def ensure_indexes():
    try:
        coll.create_index([("created_at", ASCENDING)], name="created_at_idx")
        coll.create_index([("owasp_top", ASCENDING)], name="owasp_top_idx")
        coll.create_index([("title", TEXT), ("content", TEXT)], name="text_all", default_language="english")
    except Exception as e:
        print("[Index] create index error:", e)

ensure_indexes()
print(f"[DB] connect to(main): {MONGO_URL}  db={MONGO_DB}  coll={MONGO_COLL}")

# =========================
# åˆ†æå°ˆç”¨è³‡æ–™æºï¼ˆè®€è¨“ç·´åº«ï¼šwebcommentIT_train.comment_trainï¼‰
# =========================
ANALYSIS_DB   = os.environ.get("ANALYSIS_DB",   "webcommentIT_train")
ANALYSIS_COLL = os.environ.get("ANALYSIS_COLL", "comment_train")
ANALYSIS_CONF_MIN = float(os.environ.get("ANALYSIS_CONF_MIN", "0.0"))  # å¦‚éœ€ä¿¡å¿ƒé–€æª»å¯è¨­ 0.4

mdb_an  = mongo_client[ANALYSIS_DB]
coll_an = mdb_an[ANALYSIS_COLL]
OWASP_LABELS = [f"A{i}" for i in range(1, 11)]  # A1~A10

print(f"[DB] connect to(analysis): {MONGO_URL}  db={ANALYSIS_DB}  coll={ANALYSIS_COLL}")

# =========================
# App ç‹€æ…‹æŒä¹…åŒ–ï¼ˆå­˜åˆ°ä¸»åº«çš„ _app_stateï¼‰
# =========================
STATE_COLL = db["_app_state"]  # å­˜åœ¨ webcommentIT è£¡

def _state_get() -> dict:
    doc = STATE_COLL.find_one({"_id": "app_state"}) or {}
    return doc.get("data", {})

def _state_set(patch: dict):
    cur = _state_get()
    cur.update(patch or {})
    STATE_COLL.update_one(
        {"_id": "app_state"},
        {"$set": {"data": cur}},
        upsert=True
    )
    return cur

# =========================
# ä»»å‹™ / ç‹€æ…‹ï¼ˆè¨˜æ†¶é«” + æŒä¹…åŒ–ï¼‰
# =========================
TASKS: Dict[str, Any] = {}
LAST_CRAWL_AT: Optional[datetime] = None

# ä¸‰å€‹ä¾†æºçš„é–‹é—œç‹€æ…‹ï¼ˆé è¨­é—œé–‰ï¼‰
SOURCES = {"hn": False, "reddit": False, "stackex": False}
def enabled_sources_list() -> List[str]:
    return [k for k, v in SOURCES.items() if v]

# =========================
# APScheduler
# =========================
class Config:
    SCHEDULER_API_ENABLED = True
app.config.from_object(Config())
scheduler = APScheduler()
scheduler.init_app(app)
scheduler.start()

# =========================
# å°å·¥å…·
# =========================
SAFE_ID = re.compile(r"^[\w\-]+$")

def minutes_sanitized(val: int, mn: int = 1, mx: int = 24*60*7) -> int:
    try:
        val = int(val)
    except Exception:
        val = 60
    return max(mn, min(mx, val))

def template_exists(name: str) -> bool:
    try:
        app.jinja_loader.get_source(app.jinja_env, name)
        return True
    except Exception:
        return False

def render_first(*candidates, **ctx):
    """ä¾åºå˜—è©¦å¤šå€‹æ¨¡æ¿ï¼›æ‰¾ä¸åˆ°æ™‚å›å¯è®€æç¤ºè€Œé 500"""
    for name in candidates:
        if template_exists(name):
            try:
                return render_template(name, **ctx)
            except Exception as e:
                return render_template_string(
                    "<h3>æ¨¡æ¿æ¸²æŸ“éŒ¯èª¤</h3><pre>{{ err }}</pre>", err=str(e)
                )
    missing = "ã€".join(candidates)
    return render_template_string(f"<h3>ç¼ºå°‘æ¨¡æ¿</h3><p>è«‹æä¾›ï¼š{missing}</p>")

# åœ–è¡¨ï¼ˆECharts option JSONï¼‰
CHART_DIR = os.path.join("static", "charts")
os.makedirs(CHART_DIR, exist_ok=True)

def chart_path(chart_id: str) -> str:
    if not SAFE_ID.match(chart_id):
        raise ValueError("bad chart id")
    return os.path.join(CHART_DIR, f"{chart_id}.json")

def list_charts_meta() -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    for fp in glob(os.path.join(CHART_DIR, "*.json")):
        try:
            with open(fp, "r", encoding="utf-8") as f:
                cfg = json.load(f)
            stat = os.stat(fp)
            items.append({
                "id": cfg.get("id") or os.path.splitext(os.path.basename(fp))[0],
                "title": cfg.get("title") or "Chart",
                "mtime": stat.st_mtime,
                "created_at": datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S"),
            })
        except Exception:
            continue
    items.sort(key=lambda x: x["mtime"], reverse=True)
    return items

def latest_chart_meta():
    items = list_charts_meta()
    return items[0] if items else None

# =========================
# çµ±ä¸€ï¼šé€é unified çˆ¬èŸ²åŸ·è¡Œï¼ˆä¸²æµå›å ±ï¼‰
# =========================
from subprocess import Popen, PIPE

PY_BIN = os.environ.get("PYTHON_BIN", sys.executable)
CRAWLER_TIMEOUT = int(os.environ.get("CRAWLER_TIMEOUT", "1800"))

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CRAWLER_PATH = os.path.join(BASE_DIR, "crawler_unified.py")  # æ˜ç¢ºæŒ‡å‘æœ¬å°ˆæ¡ˆå…§çš„æª”æ¡ˆ

def _run_proc_stream(cmd: list, name: str, timeout_sec: int, on_line=None) -> bool:
    """
    ä¸²æµè®€å­ç¨‹åº stdout/stderrï¼ˆé€è¡Œï¼‰ï¼Œç¢ºä¿ç„¡ç·©è¡ï¼ˆ-uï¼‰
    cmd åƒ…åŒ…å«åƒæ•¸ï¼›å¯¦éš›åŸ·è¡Œæª”èˆ‡ -u åœ¨æ­¤å‡½å¼è™•ç†ã€‚
    """
    if not os.path.exists(CRAWLER_PATH):
        if on_line:
            on_line(f"[error] æ‰¾ä¸åˆ° {CRAWLER_PATH}", stream="stderr")
        return False

    real_cmd = [PY_BIN, "-u", CRAWLER_PATH] + cmd
    print(f"[crawler] start {name}: {' '.join([str(x) for x in real_cmd])}")

    p = Popen(
        real_cmd,
        cwd=BASE_DIR,
        stdout=PIPE,
        stderr=PIPE,
        bufsize=1,              # è¡Œç·©è¡
        universal_newlines=True # text mode
    )

    try:
        # å…ˆè®€ stdout
        for line in iter(p.stdout.readline, ""):
            if not line:
                break
            if on_line:
                on_line(line.rstrip("\n"), stream="stdout")
        # å†è®€ stderrï¼ˆé¿å…é˜»å¡ï¼‰
        for line in iter(p.stderr.readline, ""):
            if not line:
                break
            if on_line:
                on_line(line.rstrip("\n"), stream="stderr")
    except Exception as e:
        if on_line:
            on_line(f"[error] {e}", stream="stderr")

    rc = p.wait(timeout=timeout_sec)
    print(f"[crawler] {name} rc={rc}")
    return rc == 0

def run_unified_only(sources: List[str], mode: str = None, progress: Dict[str, Any] = None) -> bool:
    """
    å‘¼å« crawler_unified.pyï¼ˆæ”¯æ´ --only å¤šå€¼èˆ‡åˆ¥åï¼‰ï¼Œå³æ™‚è§£æ stdout æ›´æ–°é€²åº¦ã€‚
    æœƒå…ˆæŠŠå„ä¾†æºç‹€æ…‹è¨­ç‚º starting â†’ é¿å… UI ç©ºç™½ã€‚
    """
    alias = {"se": "stackex", "stackexchange": "stackex", "stack": "stackex"}
    only: List[str] = []
    for s in (sources or []):
        if not s:
            continue
        k = str(s).lower().strip()
        only.append(alias.get(k, k))

    mode = (mode or os.environ.get("CRAWLER_MODE") or "default").strip()

    # åˆå§‹åŒ–é€²åº¦
    if progress is not None:
        progress.setdefault("phase", "crawling")
        progress.setdefault("sources", only or ["all"])
        progress.setdefault("counts", {"processed": 0, "written": 0})
        progress.setdefault("per_source", {})
        progress.setdefault("log", [])
        for s in (only or ["all"]):
            progress["per_source"].setdefault(s, {
                "posts_total": 0, "posts_inc": 0,
                "comments_total": 0, "comments_inc": 0,
                "page_cur": 0, "page_max": 0, "status": "starting"
            })

    # å­è¡Œç¨‹åªåƒåƒæ•¸ï¼›å¯¦éš›è·¯å¾‘èˆ‡ -u åœ¨ _run_proc_stream è™•ç†
    args = ["--mode", mode, "--include-comments"]
    if only:
        args += ["--only", ",".join(only)]

    # æ­£å‰‡ï¼šæŠ“ä¾†æº/é æ•¸/çµ±è¨ˆ/åˆ†é¡
    rx_source = re.compile(r"^\s*\[(hn|reddit|stackex|stackexchange)\]\s*", re.I)
    rx_page   = re.compile(r"(?:^|\s)page\s*[:=]\s*(\d+)\s*/\s*(\d+)", re.I)
    rx_counts = re.compile(
        r"posts\s*[:=]\s*(\d+)\s*(?:\(\+?|\+)\s*(\d+)\)?"
        r".*?(comments|replies)\s*[:=]\s*(\d+)\s*(?:\(\+?|\+)\s*(\d+)\)?",
        re.I,
    )
    rx_class  = re.compile(
        r"classif(?:y|ication).*(?:processed|proc)\s*[:=]\s*(\d+)\s*(?:\(\+?|\+)\s*(\d+)\)?.*?"
        r"(?:written|train|insert)\s*[:=]\s*(\d+)\s*(?:\(\+?|\+)\s*(\d+)\)?",
        re.I,
    )

    cur_src = None
    def on_line(line: str, stream="stdout"):
        nonlocal cur_src
        if progress is None:
            return
        # ring log
        log = progress["log"]
        tag = "E" if stream == "stderr" else " "
        log.append(f"{tag} {line}")
        if len(log) > 200:
            del log[:len(log) - 200]

        # ä¾†æºåˆ‡æ›ï¼ˆå¦‚ "[hn] ..."ï¼‰
        m = rx_source.search(line)
        if m:
            name = m.group(1).lower()
            cur_src = "stackex" if name.startswith("stack") else name
            progress["current_source"] = cur_src
            progress["per_source"].setdefault(cur_src, {
                "posts_total": 0, "posts_inc": 0,
                "comments_total": 0, "comments_inc": 0,
                "page_cur": 0, "page_max": 0, "status": "crawling"
            })
        # é æ•¸
        m = rx_page.search(line)
        if m and cur_src:
            cur, total = int(m.group(1)), int(m.group(2))
            ps = progress["per_source"][cur_src]
            ps.update({"page_cur": cur, "page_max": total, "status": "crawling"})
        # çµ±è¨ˆ
        m = rx_counts.search(line)
        if m and cur_src:
            posts_total = int(m.group(1))
            posts_added = int(m.group(2))
            comments_total = int(m.group(4))
            comments_added = int(m.group(5))
            ps = progress["per_source"][cur_src]
            ps.update({
                "posts_total": posts_total,
                "posts_inc": posts_added,
                "comments_total": comments_total,
                "comments_inc": comments_added,
                "status": "crawling"
            })
        # åˆ†é¡é€²åº¦
        m = rx_class.search(line)
        if m:
            processed_total, processed_inc, written_total, written_inc = map(int, m.groups())
            progress["counts"]["processed"] += processed_inc
            progress["counts"]["written"]  += written_inc
            progress["phase"] = "label"

        # å®Œæˆæç¤ºï¼ˆè‹¥å­ç¨‹å¼æœ‰å°ï¼‰
        if cur_src and (" done" in line.lower() or " finished" in line.lower()):
            progress["per_source"][cur_src]["status"] = "done"

    ok = _run_proc_stream(args, f"unified({mode}|only={','.join(only) if only else 'all'})", CRAWLER_TIMEOUT, on_line)
    if progress is not None:
        progress["phase"] = "crawl_done"
        for s, v in progress["per_source"].items():
            if v.get("status") in ("starting", "crawling"):
                v["status"] = "done" if ok else "error"
    return ok

def _run_enabled_sources() -> List[tuple]:
    """ä¾ UI é–‹é—œï¼Œçµ±ä¸€åªå‘¼å« unifiedï¼›ç§»é™¤èˆŠçš„å–®ç«™åŸ·è¡Œè·¯å¾‘"""
    chosen = enabled_sources_list()
    if not chosen:
        print("  ï¼ˆç„¡å•Ÿç”¨ä¾†æºï¼Œç•¥éï¼‰")
        return []
    ok = run_unified_only(chosen, os.environ.get("CRAWLER_MODE", "default"), progress=TASKS.get("last_refresh"))
    return [("unified", ok)]

# =========================
# åˆ†æåœ–è¡¨ï¼ˆæ”¹è®€è¨“ç·´åº« A1~A10 / auto_labelï¼‰
# =========================
def create_owasp_top_chart() -> Optional[str]:
    """
    ç”¢ç”Ÿ ECharts åœ–è¡¨è¨­å®šæª”åˆ° static/charts/*.json
    è³‡æ–™ä¾†æºï¼šwebcommentIT_train.comment_trainï¼ˆcoll_anï¼‰
    çµ±è¨ˆæ¬„ä½ï¼šauto_labelï¼ˆåƒ… A1~A10ï¼‰
    """
    try:
        match = {"auto_label": {"$in": OWASP_LABELS}}
        # å¦‚éœ€ä¿¡å¿ƒé–€æª»è«‹è§£é™¤ä¸‹ä¸€è¡Œè¨»è§£ï¼š
        # match["$expr"] = {"$gte": [{"$ifNull": ["$auto_conf", 1.0]}, ANALYSIS_CONF_MIN]}

        pipeline = [
            {"$match": match},
            {"$group": {"_id": "$auto_label", "count": {"$sum": 1}}},
        ]
        rows = list(coll_an.aggregate(pipeline))
        have = {r["_id"]: int(r["count"]) for r in rows}
        labels = OWASP_LABELS
        values = [have.get(k, 0) for k in labels]

        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        chart_id = f"owasp_top_stats_{ts}"

        option = {
            "title": {"text": "OWASP Top åˆ†ä½ˆï¼ˆä¾†æºï¼šcomment_trainï¼‰"},
            "tooltip": {"trigger": "axis"},
            "toolbox": {"feature": {"saveAsImage": {}, "dataView": {}, "restore": {}, "dataZoom": {}}},
            "grid": {"left": 40, "right": 20, "bottom": 60, "top": 50, "containLabel": True},
            "dataZoom": [{"type": "inside"}, {"type": "slider"}],
            "xAxis": {"type": "category", "data": labels, "axisLabel": {"rotate": 45}},
            "yAxis": {"type": "value"},
            "series": [{"type": "bar", "name": "Count", "data": values, "barMaxWidth": 40}],
        }
        cfg = {"id": chart_id, "title": "OWASP Top åˆ†ä½ˆ", "option": option}
        with open(chart_path(chart_id), "w", encoding="utf-8") as f:
            json.dump(cfg, f, ensure_ascii=False, indent=2)
        return chart_id
    except Exception as e:
        print("[create_owasp_top_chart] error:", e)
        return None

# =========================
# Pages
# =========================
@app.route("/")
def index():
    return dashboard()

@app.route("/dashboard")
def dashboard():
    latest = latest_chart_meta()
    job = scheduler.get_job("crawler_job")
    schedule = None
    if job:
        try:
            interval_min = int(getattr(job.trigger, "interval").total_seconds() // 60)
        except Exception:
            interval_min = None
        schedule = {
            "minutes": interval_min,
            "next": job.next_run_time.strftime("%Y-%m-%d %H:%M:%S") if job.next_run_time else "-"
        }
    last_crawl_iso = LAST_CRAWL_AT.strftime("%Y-%m-%d %H:%M:%S") if LAST_CRAWL_AT else None

    return render_first(
        "manage/dashboard.html", "dashboard.html",
        latest_chart=latest, schedule=schedule, last_crawl_at=last_crawl_iso
    )

@app.route("/manage/system")
def manage_system():
    return render_first("manage/system_ops.html", "system_ops.html")

@app.route("/manage/analysis")
def manage_analysis():
    return render_first("manage/data_analysis.html", "data_analysis.html")

@app.route("/manage/crawler")
def manage_crawler():
    job = scheduler.get_job("crawler_job")
    schedule = None
    if job:
        try:
            interval_min = int(getattr(job.trigger, "interval").total_seconds() // 60)
        except Exception:
            interval_min = None
        schedule = {
            "minutes": interval_min,
            "next": job.next_run_time.strftime("%Y-%m-%d %H:%M:%S") if job.next_run_time else "-"
        }
    return render_first("manage/crawler_control.html", "crawler_control.html",
                        schedule=schedule, sources=SOURCES, enabled_list=enabled_sources_list())

@app.route("/manage/ux")
def manage_ux():
    return render_first("manage/user_experience.html", "user_experience.html")

# =========================
# ç³»çµ±ç¶­é‹
# =========================
@app.route("/ops/clear", methods=["POST"], endpoint="clear_data")
def clear_data():
    try:
        res = coll.delete_many({})
        flash(f"ğŸ—‘ï¸ å·²æ¸…ç©º {MONGO_DB}.{MONGO_COLL}ï¼Œåˆªé™¤ {res.deleted_count} ç­†")
    except Exception as e:
        flash(f"âŒ æ¸…ç©ºå¤±æ•—ï¼š{e}")
    return redirect(url_for("manage_system"))

@app.route("/ops/export", methods=["POST"], endpoint="export_data")
def export_data():
    limit = int(request.form.get("limit", 50000))
    cur = coll.find({}, projection={"_id": 0}).limit(limit)

    buf = io.StringIO()
    writer = None
    count = 0
    for doc in cur:
        if writer is None:
            headers = sorted(doc.keys())
            writer = csv.DictWriter(buf, fieldnames=headers, extrasaction="ignore")
            writer.writeheader()
        row = {k: (v if isinstance(v, (str, int, float, type(None))) else str(v)) for k, v in doc.items()}
        writer.writerow(row)
        count += 1

    byte_buf = io.BytesIO(buf.getvalue().encode("utf-8-sig"))
    fname = f"{MONGO_DB}_{MONGO_COLL}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    flash(f"ğŸ“ å·²åŒ¯å‡º {count} ç­†è³‡æ–™ç‚º CSV")
    return send_file(byte_buf, as_attachment=True, download_name=fname, mimetype="text/csv")

@app.route("/ops/refresh_cache", methods=["POST"], endpoint="refresh_cache")
def ops_refresh_cache():
    flash("âœ… å¿«å–å·²é‡æ–°æ•´ç†ï¼ˆç¤ºç¯„ï¼‰")
    return redirect(url_for("manage_system"))

# å°å·¥å…·ï¼šæª¢æŸ¥ç›®å‰ DB èˆ‡ç­†æ•¸
@app.route("/api/db_info")
def api_db_info():
    return jsonify({
        "url": MONGO_URL, "db": MONGO_DB, "coll": MONGO_COLL,
        "count": coll.count_documents({})
    })

# =========================
# åˆ†æï¼šåœ–è¡¨ / è¶¨å‹¢ / æœå°‹ï¼ˆæ”¹è®€ coll_an + auto_labelï¼‰
# =========================
@app.route("/analysis/generate")
def generate_charts():
    cid = create_owasp_top_chart()
    flash("ğŸ“‘ å·²ç”¢ç”Ÿäº’å‹•å¼åœ–è¡¨" if cid else "âŒ ç”¢ç”Ÿåœ–è¡¨å¤±æ•—")
    return redirect(url_for("manage_analysis"))

@app.route("/analysis/trends")
def view_trends():
    """
    æ”¹è®€ coll_anï¼ˆè¨“ç·´åº«ï¼‰ï¼Œä»¥ auto_label(A1~A10) ç‚ºé¡åˆ¥ï¼Œ
    ä¾ day é€²è¡Œåˆ†çµ„çµ±è¨ˆï¼›Accept: application/json å‰‡å›è³‡æ–™ï¼Œå¦å‰‡ render é é¢ã€‚
    """
    wants_json = request.accept_mimetypes.best == "application/json"
    days = minutes_sanitized(int(request.args.get("days", 7)), mn=1, mx=90)

    if wants_json:
        try:
            start_dt = datetime.combine(date.today() - timedelta(days=days - 1), datetime.min.time())
            match = {
                "auto_label": {"$in": OWASP_LABELS},
                "created_at": {"$gte": start_dt},
            }
            # å¦‚éœ€ä¿¡å¿ƒé–€æª»è«‹è§£é™¤ä¸‹ä¸€è¡Œè¨»è§£ï¼š
            # match["$expr"] = {"$gte": [{"$ifNull": ["$auto_conf", 1.0]}, ANALYSIS_CONF_MIN]}

            pipeline = [
                {"$match": match},
                {"$project": {"day": {"$dateToString": {"format": "%m/%d", "date": "$created_at"}}, "cls": "$auto_label"}},
                {"$group": {"_id": {"day": "$day", "cls": "$cls"}, "c": {"$sum": 1}}},
                {"$sort": {"_id.day": 1}}
            ]
            rows = list(coll_an.aggregate(pipeline))
            labels = [(date.today() - timedelta(days=i)).strftime("%m/%d") for i in range(days - 1, -1, -1)]
            labels_set = set(labels)
            by_cls: Dict[str, Dict[str, int]] = {}
            for r in rows:
                d = r["_id"]["day"]
                cls = r["_id"].get("cls") or "Uncategorized"
                if d not in labels_set:
                    continue
                by_cls.setdefault(cls, {lab: 0 for lab in labels})
                by_cls[cls][d] = int(r["c"])
            datasets = []
            order = {f"A{i}": i for i in range(1, 11)}
            for cls in sorted(by_cls.keys(), key=lambda k: order.get(k, 99)):
                day_map = by_cls[cls]
                datasets.append({"label": cls, "data": [day_map[lab] for lab in labels]})
            return jsonify({"labels": labels, "datasets": datasets})
        except Exception as e:
            print("[trends] error:", e)
            return jsonify({"labels": [], "datasets": []})

    return render_first("manage/trends.html", "trends.html")

@app.route("/search")
def search():
    q = (request.args.get("q") or "").strip()
    if not q:
        flash("ğŸ” æœå°‹ï¼šé—œéµå­—ç‚ºç©º")
        return redirect(url_for("manage_analysis"))
    try:
        cursor = coll.find(
            {"$text": {"$search": q}},
            {"score": {"$meta": "textScore"}, "title": 1, "content": 1, "url": 1}
        ).sort([("score", {"$meta": "textScore"})]).limit(50)
        count = len(list(cursor))
        flash(f"ğŸ” æœå°‹ã€Œ{q}ã€æ‰¾åˆ° {count} ç­†ï¼ˆtop 50ï¼‰")
    except Exception:
        count = coll.count_documents({"content": {"$regex": q, "$options": "i"}})
        flash(f"ğŸ” æœå°‹ã€Œ{q}ï¼‰ã€æ‰¾åˆ° {count} ç­†ï¼ˆregexï¼‰")
    return redirect(url_for("manage_analysis"))

# =========================
# /api/statsï¼ˆæ”¹è®€è¨“ç·´åº« A1~A10 / auto_labelï¼‰
# =========================
@app.route("/api/stats")
def api_stats():
    try:
        match = {"auto_label": {"$in": OWASP_LABELS}}
        # å¦‚éœ€ä¿¡å¿ƒé–€æª»è«‹è§£é™¤ä¸‹ä¸€è¡Œè¨»è§£ï¼š
        # match["$expr"] = {"$gte": [{"$ifNull": ["$auto_conf", 1.0]}, ANALYSIS_CONF_MIN]}

        pipeline = [
            {"$match": match},
            {"$group": {"_id": "$auto_label", "count": {"$sum": 1}}}
        ]
        rows = list(coll_an.aggregate(pipeline))
        have = {r["_id"]: int(r["count"]) for r in rows}
        by_class = [{"class": lab, "count": have.get(lab, 0)} for lab in OWASP_LABELS]
        return jsonify({"ok": True, "by_class": by_class})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

# =========================
# å–å¾—æ’ç¨‹è³‡è¨Šï¼ˆå€’æ•¸è¨ˆæ™‚ç”¨ï¼‰
# =========================
@app.get("/api/schedule_info")
def api_schedule_info():
    try:
        job = scheduler.get_job("crawler_job")
        enabled = job is not None
        minutes = None
        next_epoch = None
        next_iso = None

        if enabled:
            try:
                td = getattr(job.trigger, "interval", None)
                if td:
                    minutes = int(td.total_seconds() // 60)
            except Exception:
                minutes = None

            if job.next_run_time:
                nxt = job.next_run_time
                next_epoch = int(nxt.timestamp() * 1000)  # çµ¦å‰ç«¯åšå€’æ•¸
                next_iso = nxt.strftime("%Y-%m-%d %H:%M:%S")

        return jsonify({
            "ok": True,
            "enabled": enabled,
            "minutes": minutes,
            "next_epoch": next_epoch,
            "next_iso": next_iso,
            "server_now_epoch": int(datetime.now().timestamp() * 1000),
        })
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

# =========================
# åœ–è¡¨ APIï¼ˆEChartsï¼‰
# =========================
@app.route("/api/charts")
def api_charts():
    return jsonify({"ok": True, "items": list_charts_meta()})

@app.route("/api/chart/<chart_id>")
def api_chart(chart_id: str):
    try:
        with open(chart_path(chart_id), "r", encoding="utf-8") as f:
            return jsonify(json.load(f))
    except Exception as e:
        return jsonify({"error": str(e)}), 404

@app.route("/api/charts/delete", methods=["POST"])
def api_charts_delete():
    data = request.get_json(silent=True) or {}
    chart_id = (data.get("id") or "").strip()
    if not chart_id or not SAFE_ID.match(chart_id):
        return jsonify({"ok": False, "error": "invalid id"}), 400
    try:
        os.remove(chart_path(chart_id))
        return jsonify({"ok": True})
    except FileNotFoundError:
        return jsonify({"ok": False, "error": "not found"}), 404
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

# =========================
# ä¾†æºé–‹é—œï¼ˆåŠ ï¼šæŒä¹…åŒ–ï¼‰
# =========================
@app.route("/crawler/source/<src>/toggle", methods=["POST"])
def toggle_source(src: str):
    if src not in SOURCES:
        flash("âš ï¸ æœªçŸ¥ä¾†æº")
        return redirect(url_for("manage_crawler"))
    SOURCES[src] = not SOURCES[src]
    _state_set({"sources": SOURCES})  # â† å¯«å›æŒä¹…åŒ–
    state = "å·²é–‹å•Ÿ" if SOURCES[src] else "å·²é—œé–‰"
    flash(f"ä¾†æº {src}ï¼š{state}")
    return redirect(url_for("manage_crawler"))

@app.route("/api/sources")
def api_sources():
    return jsonify({"ok": True, "sources": SOURCES, "enabled": enabled_sources_list()})

# =========================
# å³æ™‚æ›´æ–°ï¼ˆèƒŒæ™¯ï¼‰
# =========================
def _touch_last_crawl():
    global LAST_CRAWL_AT
    LAST_CRAWL_AT = datetime.now()

def do_refresh_now_task(sources_override: Optional[List[str]] = None, mode: Optional[str] = None):
    """å¯æ¥å— sources_override ç›´æ¥è·‘ unifiedï¼›è‹¥æœªæä¾›å‰‡ä¾ UI é–‹é—œ"""
    progress: Dict[str, Any] = {
        "status": "running",
        "phase": "init",
        "log": [],
        "counts": {"processed": 0, "written": 0},
        "per_source": {},
        "sources": sources_override or enabled_sources_list(),
    }
    TASKS["last_refresh"] = progress
    try:
        print("â±ï¸ ç«‹å³æ›´æ–°ï¼šé–‹å§‹  sources_override=", sources_override, " mode=", mode)
        before = coll.count_documents({})

        ok = run_unified_only(progress["sources"], mode or os.environ.get("CRAWLER_MODE", "default"), progress=progress)
        results = [("unified", ok)]

        _touch_last_crawl()
        _state_set({"last_crawl_at": LAST_CRAWL_AT.isoformat()})  # â† æŒä¹…åŒ–ã€Œä¸Šæ¬¡æ›´æ–°ã€
        chart_id = create_owasp_top_chart()  # ç”¢åœ–æ”¹è®€è¨“ç·´åº«
        after = coll.count_documents({})
        delta = after - before

        progress.update({
            "status": "done",
            "phase": "done",
            "results": results,
            "chart_id": chart_id,
            "finished_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "added": int(delta)
        })
        print("â±ï¸ ç«‹å³æ›´æ–°ï¼šå®Œæˆ", results)
    except Exception as e:
        progress.update({"status": "error", "error": str(e)})
        print("â±ï¸ ç«‹å³æ›´æ–°ï¼šéŒ¯èª¤", e)

@app.route("/crawler/refresh_now", methods=["POST"])
def refresh_now():
    if not enabled_sources_list():
        flash("âš ï¸ å°šæœªé–‹å•Ÿä»»ä½•ä¾†æºï¼Œè«‹å…ˆåœ¨ä¸Šæ–¹å¡ç‰‡ã€Œé–‹å•Ÿã€è‡³å°‘ä¸€å€‹ä¾†æº")
        return redirect(url_for("manage_crawler"))
    TASKS["last_refresh"] = {"status": "running", "started_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"), "log": [], "counts": {"processed":0,"written":0}, "per_source": {}}
    Thread(target=do_refresh_now_task, daemon=True).start()
    flash("â±ï¸ å·²è§¸ç™¼å³æ™‚æ›´æ–°ï¼ˆèƒŒæ™¯åŸ·è¡Œï¼‰ï¼Œå®Œæˆå¾Œé¦–é æœƒå‡ºç¾æœ€æ–°åœ–è¡¨")
    return redirect(url_for("manage_crawler"))

@app.route("/api/refresh_now", methods=["POST"])
def refresh_now_api():
    """
    JSON bodyï¼ˆå¯é¸ï¼‰:
    {
      "mode": "default" | "refresh" | "backfill",
      "sources": ["hn","reddit","stackex"]  // è‹¥æä¾›ï¼Œå°‡å¿½ç•¥ UI é–‹é—œï¼›è‹¥æœªæä¾›ï¼Œå°‡ä½¿ç”¨å·²é–‹å•Ÿä¾†æº
    }
    """
    data = request.get_json(silent=True) or {}
    mode = (data.get("mode") or os.environ.get("CRAWLER_MODE") or "default").strip()
    sources = data.get("sources")

    # è‹¥æ²’æä¾› sources ä¸” UI ä¹Ÿæ²’é–‹å•Ÿä»»ä½•ä¾†æº â†’ å› 400
    if not sources and not enabled_sources_list():
        return jsonify({"ok": False, "error": "no_sources", "message": "è«‹æŒ‡å®š sources æˆ–å…ˆé–‹å•Ÿè‡³å°‘ä¸€å€‹ä¾†æº"}), 400

    TASKS["last_refresh"] = {"status": "running", "started_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"), "log": [], "counts": {"processed":0,"written":0}, "per_source": {}}
    Thread(target=do_refresh_now_task, kwargs={"sources_override": sources or enabled_sources_list(), "mode": mode}, daemon=True).start()
    return jsonify({"ok": True, "message": "å·²è§¸ç™¼å³æ™‚æ›´æ–°ï¼ˆèƒŒæ™¯åŸ·è¡Œï¼‰", "mode": mode, "sources": sources or enabled_sources_list()})

def _compose_status_text(info: Dict[str, Any]) -> str:
    lines = []
    lines.append(f"ç‹€æ…‹ï¼š{info.get('status','idle')}ï¼ˆéšæ®µï¼š{info.get('phase','-')}ï¼‰")
    per = info.get("per_source") or {}
    order = ["hn", "reddit", "stackex"]
    for s in order:
        if s not in per: 
            continue
        v = per[s]
        lines.append(
            f"[{s}] è²¼æ–‡={v.get('posts_total',0)}ï¼ˆ+{v.get('posts_inc',0)}ï¼‰  "
            f"ç•™è¨€={v.get('comments_total',0)}ï¼ˆ+{v.get('comments_inc',0)}ï¼‰  "
            f"é ={v.get('page_cur',0)}/{v.get('page_max',0)}  ç‹€æ…‹={v.get('status','')}"
        )
    counts = info.get("counts") or {}
    lines.append(f"åˆ†é¡ï¼šprocessed={counts.get('processed',0)} / å¯«å…¥è¨“ç·´ +{counts.get('written',0)}")
    return "\n".join(lines)

@app.route("/api/refresh_status")
def refresh_status():
    info = TASKS.get("last_refresh", {"status": "idle"})
    log_tail = (info.get("log") or [])[-80:]
    return jsonify({
        "ok": True,
        "status": info.get("status", "idle"),
        "phase": info.get("phase", "idle"),
        "sources": info.get("sources"),
        "current_source": info.get("current_source"),
        "page": info.get("page"),
        "counts": info.get("counts", {}),
        "per_source": info.get("per_source", {}),
        "status_text": _compose_status_text(info),
        "results": info.get("results"),
        "chart_id": info.get("chart_id"),
        "finished_at": info.get("finished_at"),
        "added": info.get("added"),
        "last_crawl_at": LAST_CRAWL_AT.strftime("%Y-%m-%d %H:%M:%S") if LAST_CRAWL_AT else None,
        "log_tail": log_tail,
    })

# =========================
# æ’ç¨‹ï¼ˆå®šæœŸæ›´æ–°ï¼‰+ å•Ÿå‹•æ™‚å›å¡«
# =========================
def scheduled_crawl():
    print("â° æ’ç¨‹è§¸ç™¼")
    TASKS["last_refresh"] = {"status": "running", "phase": "init", "log": [], "counts": {"processed":0,"written":0}, "per_source": {}}
    _run_enabled_sources()
    _touch_last_crawl()
    _state_set({"last_crawl_at": LAST_CRAWL_AT.isoformat()})  # â† æŒä¹…åŒ–
    TASKS["last_refresh"].update({"status": "done", "phase": "done", "finished_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")})

def _rehydrate_from_state():
    """é‡å•Ÿä¼ºæœå™¨æ™‚ï¼Œå¾ Mongo é‚„åŸ SOURCES / LAST_CRAWL_AT / Scheduler"""
    global SOURCES, LAST_CRAWL_AT
    st = _state_get()

    # ä¾†æºé–‹é—œ
    if "sources" in st and isinstance(st["sources"], dict):
        for k in SOURCES.keys():
            if k in st["sources"]:
                SOURCES[k] = bool(st["sources"][k])

    # ä¸Šæ¬¡æ›´æ–°æ™‚é–“
    try:
        if st.get("last_crawl_at"):
            LAST_CRAWL_AT = datetime.fromisoformat(st["last_crawl_at"])
    except Exception:
        LAST_CRAWL_AT = None

    # æ’ç¨‹
    sch = st.get("scheduler") or {}
    enabled = bool(sch.get("enabled"))
    minutes = int(sch.get("minutes") or 60)
    old = scheduler.get_job("crawler_job")
    if old:
        scheduler.remove_job("crawler_job")
    if enabled:
        scheduler.add_job(id="crawler_job", func=scheduled_crawl, trigger="interval", minutes=minutes)

# å•Ÿå‹•å¾Œç«‹å³å›å¡«ç‹€æ…‹ï¼ˆéœ€åœ¨ scheduled_crawl å®šç¾©ä¹‹å¾Œï¼‰
_rehydrate_from_state()

@app.route("/crawler/schedule/start", methods=["POST"])
def start_schedule():
    preset = (request.form.get("preset") or "").strip()
    custom_value = (request.form.get("custom_value") or "").strip()
    custom_unit = (request.form.get("custom_unit") or "minutes").strip()

    minutes = 60
    if preset and preset != "custom":
        if preset.endswith("m"):
            minutes = minutes_sanitized(preset[:-1])
        elif preset.endswith("h"):
            minutes = minutes_sanitized(int(preset[:-1]) * 60)
    elif preset == "custom" and custom_value.isdigit():
        val = int(custom_value)
        minutes = minutes_sanitized(val if custom_unit == "minutes" else val * 60)

    old = scheduler.get_job("crawler_job")
    if old:
        scheduler.remove_job("crawler_job")
    scheduler.add_job(id="crawler_job", func=scheduled_crawl, trigger="interval", minutes=minutes)

    _state_set({"scheduler": {"enabled": True, "minutes": minutes}})  # â† æŒä¹…åŒ–
    flash(f"ğŸ“… å®šæœŸæ›´æ–°å·²å•Ÿå‹•ï¼Œæ¯ {minutes} åˆ†é˜åŸ·è¡Œä¸€æ¬¡")
    return redirect(url_for("manage_crawler"))

@app.route("/crawler/schedule/stop", methods=["POST"])
def stop_schedule():
    job = scheduler.get_job("crawler_job")
    if job:
        scheduler.remove_job("crawler_job")
        _state_set({"scheduler": {"enabled": False}})  # â† æŒä¹…åŒ–
        flash("âœ‹ å®šæœŸæ›´æ–°å·²åœæ­¢")
    else:
        flash("âš ï¸ æ²’æœ‰æ­£åœ¨åŸ·è¡Œçš„å®šæœŸä»»å‹™")
    return redirect(url_for("manage_crawler"))

# =========================
# å…¶å®ƒç°¡æ˜“ API
# =========================
@app.route("/api/ux/toggle_dark", methods=["POST"])
def api_ux_toggle_dark():
    cur = bool(session.get("dark_mode", False))
    session["dark_mode"] = not cur
    return jsonify({"ok": True, "enabled": session["dark_mode"]})

@app.route("/ux/darkmode", methods=["POST"])
def toggle_dark_mode():
    cur = bool(session.get("dark_mode", False))
    session["dark_mode"] = not cur
    flash("ğŸŒ™ æ·±è‰²æ¨¡å¼ï¼šå·²åˆ‡æ› " + ("ï¼ˆæ·±è‰²ï¼‰" if session["dark_mode"] else "ï¼ˆæ·ºè‰²ï¼‰"))
    return redirect(url_for("manage_ux"))

@app.route("/api/export")
def api_export():
    return redirect(url_for("export_data"))

@app.route("/api/clear", methods=["POST"])
def api_clear():
    try:
        res = coll.delete_many({})
        return jsonify({"ok": True, "deleted": res.deleted_count, "db": MONGO_DB, "collection": MONGO_COLL})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

@app.route("/api/crawl", methods=["POST"])
def api_crawl():
    data = request.get_json(silent=True) or {}
    mode = data.get("mode", "hot")
    task_id = f"task_{datetime.now().strftime('%H%M%S%f')}"
    TASKS[task_id] = {"status": "running", "logs": [f"Started crawl: {mode}"]}
    return jsonify({"ok": True, "task_id": task_id})

@app.route("/api/classify", methods=["POST"])
def api_classify():
    _ = request.get_json(silent=True) or {}
    task_id = f"task_{datetime.now().strftime('%H%M%S%f')}"
    TASKS[task_id] = {"status": "running", "logs": ["Started classify"]}
    return jsonify({"ok": True, "task_id": task_id})

@app.route("/api/task/<task_id>")
def api_task(task_id: str):
    t = TASKS.get(task_id)
    if not t:
        return jsonify({"error": "not found"}), 404
    if t["status"] == "running" and len(t["logs"]) < 5:
        t["logs"].append("åŸ·è¡Œä¸­ â€¦")
    elif t["status"] == "running":
        t["status"] = "done"
        t["logs"].append("âœ… ä»»å‹™å®Œæˆ")
    return jsonify(t)

@app.route("/api/trends")
def api_trends():
    # ç¤ºç¯„è³‡æ–™ï¼ˆä¿ç•™ï¼‰
    days = minutes_sanitized(int(request.args.get("days", 7)), mn=1, mx=90)
    today = date.today()
    labels = [(today - timedelta(days=i)).strftime("%m/%d") for i in range(days - 1, -1, -1)]
    datasets = []
    for cat, mul in [("A01", 1), ("A02", 2), ("A03", 3)]:
        datasets.append({"label": cat, "data": [((i * mul) % 10) for i in range(days)]})
    return jsonify({"labels": labels, "datasets": datasets})

# ========= ä½ è¦æ±‚çš„çµå°¾ =========
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port, debug=True)
